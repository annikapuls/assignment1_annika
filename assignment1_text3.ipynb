{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ba9562c",
   "metadata": {},
   "source": [
    "### Assignment 1 - Text 3: Skytrain Wikipedia Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "3ad0f51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "c43b2036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a variable to hold the path to the file\n",
    "filePath = \"./skytrain.txt\"\n",
    "\n",
    "# open the file as \"r\" or read only and store this opened file in f\n",
    "with open(filePath, \"r\") as f:\n",
    "    # read the data from f and store it in the string variable \"data\"\n",
    "    text3 = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "7554fb7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8715\n"
     ]
    }
   ],
   "source": [
    "# length in words\n",
    "words= nltk.word_tokenize(text3)\n",
    "length= len(words)\n",
    "\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "3102bdc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22799770510613884\n"
     ]
    }
   ],
   "source": [
    "# lexical diversity\n",
    "lexDiversity = len(set(words)) / len(words)\n",
    "\n",
    "print(lexDiversity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "4386e97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Although most of the system is elevated, SkyTrain runs at or below grade through Downtown Vancouver, for the Vancouver portion of the Canada Line until just before it reaches Richmond at Marine Drive station, through the 2.1-kilometre (1.3 mi) tunnel used by the Millennium Line between Coquitlam and Port Moody, through the 0.6-kilometre (0.4 mi) tunnel between Columbia and Sapperton stations in New Westminster, and for short stretches in Burnaby and New Westminster.\n",
      "83\n"
     ]
    }
   ],
   "source": [
    "# longest sentence (+ word count of sentence)\n",
    "sentences = nltk.sent_tokenize(text3)\n",
    "longsent = (max(sentences, key=len))\n",
    "senwords = nltk.word_tokenize(longsent)\n",
    "length = len(senwords)\n",
    "\n",
    "print(longsent)\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "d19a50f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('.', '[')\n",
      "('of', 'the')\n",
      "(',', 'the')\n",
      "(',', 'and')\n",
      "('Millennium', 'Line')\n",
      "('Canada', 'Line')\n",
      "('Expo', 'Line')\n",
      "('.', 'The')\n",
      "('the', 'Expo')\n",
      "('the', 'Millennium')\n"
     ]
    }
   ],
   "source": [
    "# top collocations\n",
    "words = nltk.word_tokenize(text3)\n",
    "finder = BigramCollocationFinder.from_words(words)\n",
    "scorer = BigramAssocMeasures.raw_freq\n",
    "N = 10\n",
    "top_collocations = finder.nbest(scorer, N)\n",
    "\n",
    "for collocation in top_collocations:\n",
    "    print(collocation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "3d00bfa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('and', 213), ('a', 109), ('as', 45), ('at', 45), ('are', 39), ('an', 28), ('also', 16), ('along', 15), ('announced', 12), ('article', 11)]\n",
      "[('extension', 30), ('expansion', 10), ('early', 8), ('every', 7), ('end', 7), ('east', 6), ('elevated', 5), ('evasion', 5), ('estimated', 5), ('ending', 4)]\n",
      "[('in', 122), ('is', 55), ('it', 28), ('into', 11), ('its', 10), ('including', 7), ('interlined', 4), ('include', 4), ('initially', 3), ('induction', 3)]\n",
      "[('of', 196), ('on', 71), ('or', 25), ('other', 11), ('one', 11), ('only', 10), ('opening', 9), ('opened', 9), ('operating', 7), ('officers', 7)]\n",
      "[('used', 10), ('using', 8), ('use', 7), ('uses', 6), ('until', 6), ('under', 5), ('underground', 3), ('usage', 3), ('up', 3), ('upgrades', 2)]\n"
     ]
    }
   ],
   "source": [
    "def top10(myList,vowel):\n",
    "# top ten words with each vowel\n",
    "    myResult = []\n",
    "    for index in myList:\n",
    "       isVowel = False\n",
    "       for element in vowel:\n",
    "          if index.startswith(element):\n",
    "             isVowel = True\n",
    "             break\n",
    "       if isVowel:\n",
    "          myResult.append(index)\n",
    "#frequency\n",
    "    fdist = FreqDist(myResult)\n",
    "    print(fdist.most_common(10))\n",
    "    \n",
    "myList = nltk.word_tokenize(text3)\n",
    "top10(myList,\"a\")\n",
    "top10(myList,\"e\")\n",
    "top10(myList,\"i\")\n",
    "top10(myList,\"o\")\n",
    "top10(myList,\"u\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "dcc97ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "although most of the system is elev , skytrain run at or below grade through downtown vancouv , for the vancouv portion of the canada line until just befor it reach richmond at marin drive station , through the 2.1-kilometr ( 1.3 mi ) tunnel use by the millennium line between coquitlam and port moodi , through the 0.6-kilometr ( 0.4 mi ) tunnel between columbia and sapperton station in new westminst , and for short stretch in burnabi and new westminst .\n"
     ]
    }
   ],
   "source": [
    "# stemmed version of longest sentence\n",
    "# tokenize longest sentence\n",
    "token_longsent = nltk.word_tokenize(longsent)\n",
    "\n",
    "# stem words\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stemmedWords = [stemmer.stem(word) for word in token_longsent]\n",
    "\n",
    "# join the stemmed words to form the stemmed sentence\n",
    "stemmedSentence = \" \".join(stemmedWords)\n",
    "\n",
    "print(stemmedSentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
